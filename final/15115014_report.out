\BOOKMARK [1][-]{section.1}{Problem A: Cart-pole}{}% 1
\BOOKMARK [2][-]{subsection.1.1}{3 trajectories under random policy}{section.1}% 2
\BOOKMARK [2][-]{subsection.1.2}{100 runs under random policy}{section.1}% 3
\BOOKMARK [2][-]{subsection.1.3}{Batch Q-learning}{section.1}% 4
\BOOKMARK [3][-]{subsubsection.1.3.1}{Linear transform}{subsection.1.3}% 5
\BOOKMARK [3][-]{subsubsection.1.3.2}{Hidden layer with 100 units, followed by ReLU}{subsection.1.3}% 6
\BOOKMARK [2][-]{subsection.1.4}{Online Q-learning}{section.1}% 7
\BOOKMARK [2][-]{subsection.1.5}{Changing the hidden layer}{section.1}% 8
\BOOKMARK [3][-]{subsubsection.1.5.1}{Fewer units \(30\)}{subsection.1.5}% 9
\BOOKMARK [3][-]{subsubsection.1.5.2}{More units \(1000\)}{subsection.1.5}% 10
\BOOKMARK [2][-]{subsection.1.6}{Adding experiece replay}{section.1}% 11
\BOOKMARK [2][-]{subsection.1.7}{Adding a target network}{section.1}% 12
\BOOKMARK [2][-]{subsection.1.8}{Implementing a SARSA agent}{section.1}% 13
\BOOKMARK [1][-]{section.2}{Problem B: Atari Games}{}% 14
\BOOKMARK [2][-]{subsection.2.1}{Random policy}{section.2}% 15
\BOOKMARK [2][-]{subsection.2.2}{Untrained Q-network}{section.2}% 16
\BOOKMARK [2][-]{subsection.2.3}{Training agents to play Atari games}{section.2}% 17
\BOOKMARK [3][-]{subsubsection.2.3.1}{Pong}{subsection.2.3}% 18
\BOOKMARK [3][-]{subsubsection.2.3.2}{Ms Pacman}{subsection.2.3}% 19
\BOOKMARK [3][-]{subsubsection.2.3.3}{Boxing}{subsection.2.3}% 20
\BOOKMARK [2][-]{subsection.2.4}{Testing the Atari agents}{section.2}% 21
\BOOKMARK [3][-]{subsubsection.2.4.1}{Pong}{subsection.2.4}% 22
\BOOKMARK [3][-]{subsubsection.2.4.2}{Ms Pacman}{subsection.2.4}% 23
\BOOKMARK [3][-]{subsubsection.2.4.3}{Boxing}{subsection.2.4}% 24
